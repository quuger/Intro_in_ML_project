{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6dac144-4030-45b2-ac01-23b5f0169bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.48.2)\n",
      "Requirement already satisfied: torch<3,>=2.3 in ./.venv/lib/python3.12/site-packages (from bitsandbytes) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from bitsandbytes) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71935ff4-a2e6-46e1-ba35-18be85753296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ffaf94-7d73-4d9a-895b-9c26f4f4e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== НАСТРОЙКИ ==================\n",
    "MODEL_NAME = \"Qwen/Qwen3-14B\"  # Ваша модель\n",
    "DATASET_PATH = \"./dataset.json\"  # Путь к вашему датасету\n",
    "OUTPUT_DIR = \"./lora-qwen3-chat-style\"  # Куда сохранять LoRA\n",
    "SAVE_MODEL_PATH = \"./MyModel\"  # Куда сохранять полную модель\n",
    "\n",
    "# Параметры LoRA\n",
    "LORA_R = 16  # Rank\n",
    "LORA_ALPHA = 32  # Alpha\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Параметры обучения\n",
    "BATCH_SIZE = 1  # Уменьшите если не хватает памяти\n",
    "GRAD_ACCUM_STEPS = 8  # Аккумулирование градиентов\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 2048  # Максимальная длина последовательности\n",
    "\n",
    "# Квантование (обязательно для 14B на GPU с 24ГБ)\n",
    "USE_QUANTIZATION = False\n",
    "LOAD_IN_4BIT = True  # 4-битное квантование\n",
    "LOAD_IN_8BIT = False  # Альтернатива: 8-битное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ce6381-623d-42e6-8a2c-cf021fec7e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели и токенизатора...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee82834de4e40cfbc095a1479895b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cae5667a0449b8aad9c904d60beadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6af237e2c8471c83a85aa3254ee17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8db7293674847a8b4cef21ae4aa9699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b36ef4298d647bdbdd46a50cae4dd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b1ce6104f84ac0b0f267fb776e3bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a992dd485e46de892d0a28177b81ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/3.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac948bdb3c31469eab4e75f9abe32e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b335e105b3b45639c1e0cc91388d6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     tokenizer.pad_token_id = tokenizer.eos_token_id\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Загрузка модели\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Автоматическое распределение по устройствам\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mUSE_QUANTIZATION\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4971\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4968\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4969\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4970\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4971\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4973\u001b[39m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[32m   4974\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:435\u001b[39m, in \u001b[36mQwen3ForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = Qwen3Model(config)\n\u001b[32m    437\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2076\u001b[39m, in \u001b[36mPreTrainedModel.__init__\u001b[39m\u001b[34m(self, config, *inputs, **kwargs)\u001b[39m\n\u001b[32m   2072\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m   2074\u001b[39m \u001b[38;5;66;03m# Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\u001b[39;00m\n\u001b[32m   2075\u001b[39m \u001b[38;5;66;03m# setting it recursively)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2076\u001b[39m \u001b[38;5;28mself\u001b[39m.config._attn_implementation_internal = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_and_adjust_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_init_check\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   2078\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m \u001b[38;5;66;03m# for initialization of the loss\u001b[39;00m\n\u001b[32m   2081\u001b[39m loss_type = \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2686\u001b[39m, in \u001b[36mPreTrainedModel._check_and_adjust_attn_implementation\u001b[39m\u001b[34m(self, attn_implementation, is_init_check)\u001b[39m\n\u001b[32m   2684\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2685\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2686\u001b[39m     applicable_attn_implementation = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_correct_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapplicable_attn_implementation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_init_check\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m     \u001b[38;5;66;03m# preload flash attention here to allow compile with fullgraph\u001b[39;00m\n\u001b[32m   2690\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m applicable_attn_implementation.startswith(\u001b[33m\"\u001b[39m\u001b[33mflash_attention\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2714\u001b[39m, in \u001b[36mPreTrainedModel.get_correct_attn_implementation\u001b[39m\u001b[34m(self, requested_attention, is_init_check)\u001b[39m\n\u001b[32m   2712\u001b[39m \u001b[38;5;66;03m# Perform relevant checks\u001b[39;00m\n\u001b[32m   2713\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m applicable_attention == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2714\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flash_attn_2_can_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_init_check\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m applicable_attention == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_3\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2716\u001b[39m     \u001b[38;5;28mself\u001b[39m._flash_attn_3_can_dispatch(is_init_check)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Intro_in_ML_project/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2422\u001b[39m, in \u001b[36mPreTrainedModel._flash_attn_2_can_dispatch\u001b[39m\u001b[34m(self, is_init_check)\u001b[39m\n\u001b[32m   2419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2421\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m importlib.util.find_spec(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2422\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2423\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2424\u001b[39m     \u001b[38;5;66;03m# Check FA2 installed version compatibility\u001b[39;00m\n\u001b[32m   2425\u001b[39m     flash_attention_version = version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mImportError\u001b[39m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "# ================== ЗАГРУЗКА МОДЕЛИ И ТОКЕНИЗАТОРА ==================\n",
    "print(\"Загрузка модели и токенизатора...\")\n",
    "\n",
    "# Настройка квантования\n",
    "bnb_config = None\n",
    "if USE_QUANTIZATION:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=LOAD_IN_4BIT,\n",
    "        load_in_8bit=LOAD_IN_8BIT,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Установка padding token если отсутствует\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Загрузка модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Автоматическое распределение по устройствам\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if not USE_QUANTIZATION else None,\n",
    "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b386c-dbbf-474c-8216-7531898d9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ПОДГОТОВКА МОДЕЛИ К ОБУЧЕНИЮ ==================\n",
    "if USE_QUANTIZATION:\n",
    "    model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb53f0-006b-43f3-94cf-64e263a2673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== НАСТРОЙКА LoRA ==================\n",
    "print(\"Настройка LoRA...\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Покажет сколько параметров обучается"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affb8e4f-4dfd-4bef-a93b-ad78176b4edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ЗАГРУЗКА И ПОДГОТОВКА ДАТАСЕТА ==================\n",
    "print(\"Загрузка датасета...\")\n",
    "\n",
    "def load_and_prepare_dataset(file_path):\n",
    "    \"\"\"Загрузка и подготовка датасета в зависимости от формата\"\"\"\n",
    "    dataset = load_dataset('json', data_files=file_path, split='train')\n",
    "    \n",
    "    def format_conversation(example):\n",
    "        \"\"\"Форматирование примера в текст для обучения\"\"\"\n",
    "        # Вариант 1: Если данные в формате ChatML\n",
    "        if 'messages' in example:\n",
    "            text = \"\"\n",
    "            for msg in example['messages']:\n",
    "                if msg['role'] == 'user':\n",
    "                    text += f\"<|im_start|>user\\n{msg['content']}<|im_end|>\\n\"\n",
    "                else:\n",
    "                    text += f\"<|im_start|>assistant\\n{msg['content']}<|im_end|>\\n\"\n",
    "            return {'text': text}\n",
    "        \n",
    "        # Вариант 2: Если данные в инструктивном формате\n",
    "        elif 'instruction' in example and 'output' in example:\n",
    "            instruction = example['instruction']\n",
    "            input_text = example.get('input', '')\n",
    "            output = example['output']\n",
    "            \n",
    "            text = f\"<|im_start|>user\\n{instruction}\"\n",
    "            if input_text:\n",
    "                text += f\"\\n{input_text}\"\n",
    "            text += f\"<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "            return {'text': text}\n",
    "        \n",
    "        # Вариант 3: Если данные в формате context-response (для вашей задачи)\n",
    "        elif 'context' in example and 'response' in example:\n",
    "            text = f\"<|im_start|>user\\n{example['context']}<|im_end|>\\n\"\n",
    "            text += f\"<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "            return {'text': text}\n",
    "        \n",
    "        # Вариант 4: Если просто текст\n",
    "        else:\n",
    "            return {'text': str(example)}\n",
    "    \n",
    "    # Применяем форматирование\n",
    "    dataset = dataset.map(format_conversation, remove_columns=dataset.column_names)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_and_prepare_dataset(DATASET_PATH)\n",
    "\n",
    "# Разделение на тренировочную и валидационную выборки\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Размер тренировочного датасета: {len(train_dataset)}\")\n",
    "print(f\"Размер валидационного датасета: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446df69-7edb-49cc-8401-9efdbc6a67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== НАСТРОЙКА АРГУМЕНТОВ ОБУЧЕНИЯ ==================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else False,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "    report_to=\"none\",  # Можно изменить на \"wandb\" для отслеживания\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")# ================== СОЗДАНИЕ ТРЕНЕРА ==================\n",
    "print(\"Создание тренера...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,  # Не пакуем последовательности для лучшего качества\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bffd27-f480-4328-9c87-fe511cf43763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ОБУЧЕНИЕ ==================\n",
    "print(\"Начало обучения...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f34ef-3b8f-47ba-9dea-99743e8e7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== СОХРАНЕНИЕ МОДЕЛИ ==================\n",
    "print(\"Сохранение модели...\")\n",
    "\n",
    "# Способ 1: Сохранить только LoRA адаптеры (легковесный)\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"LoRA адаптеры сохранены в {OUTPUT_DIR}\")\n",
    "\n",
    "# Способ 2: Объединить LoRA с базовой моделью и сохранить полностью\n",
    "print(\"Объединение LoRA с базовой моделью...\")\n",
    "\n",
    "# Загружаем базовую модель без квантования для слияния\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Объединяем LoRA с базовой моделью\n",
    "merged_model = get_peft_model(base_model, peft_config)\n",
    "merged_model.load_state_dict(model.state_dict(), strict=False)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Сохраняем объединенную модель\n",
    "merged_model.save_pretrained(SAVE_MODEL_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(SAVE_MODEL_PATH)\n",
    "\n",
    "# Также сохраняем в формате .pt (как вы просили)\n",
    "torch.save(merged_model.state_dict(), \"./MyModel.pt\")\n",
    "print(f\"Полная модель сохранена в {SAVE_MODEL_PATH} и ./MyModel.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbae82a-5838-4f3a-9935-ab5a3b92c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== ТЕСТИРОВАНИЕ ==================\n",
    "print(\"\\nТестирование обученной модели...\")\n",
    "\n",
    "# Переводим модель в режим оценки\n",
    "merged_model.eval()\n",
    "\n",
    "# Тестовый промпт\n",
    "test_prompt = \"<|im_start|>user\\nПривет! Как дела?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(merged_model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = merged_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Ответ модели: {response}\")\n",
    "\n",
    "print(\"Обучение завершено успешно!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
